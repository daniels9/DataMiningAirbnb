{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Airbnb Prices for Munich"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of our data mining project is to predict prices for new Airbnb listings in Munich. To achieve this, we will train a regression model on existing Airbnb data from www.insideairbnb.com."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "##### [1 Preprocessing](#preprocessing)\n",
    "##### [2 Data Mining](#data_mining)\n",
    "##### [3 Interpretation and Evaluation](#interpretation_evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, GridSearchCV, train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from math import sqrt\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.feature_selection import RFECV, f_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "import itertools\n",
    "from sklearn import neighbors\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import fbeta_score, make_scorer, mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='preprocessing'></a>\n",
    "## 1 Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-24 09:36:17 : Dataset loaded and preprocessed.\n"
     ]
    }
   ],
   "source": [
    "%run modules/preprocessing.py\n",
    "preprocessed_df = load_and_preprocess_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features:  15               maximum_nights\n",
      "12             security_deposit\n",
      "13                 cleaning_fee\n",
      "1     host_total_listings_count\n",
      "10                         beds\n",
      "                ...            \n",
      "23                      heating\n",
      "11                     bed_type\n",
      "20                     internet\n",
      "34           verification_phone\n",
      "2          host_has_profile_pic\n",
      "Name: Specs, Length: 70, dtype: object\n"
     ]
    }
   ],
   "source": [
    "%run modules/preprocessing.py\n",
    "features = select_best_features(preprocessed_df, number_of_features = 70) # Total number of features: 70\n",
    "label = preprocessed_df['maximum_price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create suitable bins for stratification in train-test-split\n",
    "bins = pd.qcut(label, 30, labels=False)\n",
    "\n",
    "# Train-test-split\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, label, test_size = 0.2, random_state = 42, stratify=bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to delete outliers, execute this after the train-test-split:\n",
    "\n",
    "# %run modules/preprocessing.py\n",
    "# x_train, y_train = delete_price_outliers(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accommodates   beds   0.7060608155629018\n",
      "bedrooms   beds   0.6501529074983061\n",
      "beds   accommodates   0.7060608155629018\n",
      "beds   bedrooms   0.6501529074983061\n",
      "require_guest_profile_picture   require_guest_phone_verification   0.7871857495927765\n",
      "require_guest_phone_verification   require_guest_profile_picture   0.7871857495927765\n",
      "verification_government_id   verification_jumio   0.9765520138913213\n",
      "verification_jumio   verification_government_id   0.9765520138913213\n",
      "verification_offline_government_id   verification_selfie   0.6336336790497336\n",
      "verification_selfie   verification_offline_government_id   0.6336336790497336\n"
     ]
    }
   ],
   "source": [
    "# Check correlation of independant variables\n",
    "corr_matrix = preprocessed_df.corr()\n",
    "for (column_name, column_data) in corr_matrix.iteritems():\n",
    "    for row_name, value in column_data.iteritems():\n",
    "        if(value > 0.6 and column_name != row_name):\n",
    "            print(column_name, \" \", row_name, \" \", value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='data_mining'></a>\n",
    "## 2 Data Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Evaluation of a Dummy Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy Regressor:  -0.07154551044155429\n",
      "RMSE:  143.85943496805479\n"
     ]
    }
   ],
   "source": [
    "# k-fold cross-validation (k = 10)\n",
    "scores = []\n",
    "rmse = []\n",
    "k_fold_cross_validation = KFold(10, True, 1)\n",
    "for train_index, test_index in k_fold_cross_validation.split(features):\n",
    "\n",
    "    # Split the dataset for training and testing\n",
    "    x_train, x_test, y_train, y_test = features.loc[train_index, :], features.loc[test_index, :], label[train_index], label[test_index]\n",
    "    \n",
    "    # Dummy Regressor\n",
    "    regressor = DummyRegressor(strategy='median')\n",
    "    regressor.fit(x_train, y_train)\n",
    "\n",
    "    # Evaluation using testing dataset\n",
    "    scores.append(regressor.score(x_test, y_test))  \n",
    "    predictions = regressor.predict(x_test)\n",
    "    rmse.append(sqrt(mean_squared_error(y_test, predictions)))\n",
    "\n",
    "# Calculate performance measures\n",
    "print(\"Dummy Regressor: \", str(np.mean(scores)))\n",
    "print(\"RMSE: \", str(np.mean(rmse)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Evaluation of different Regression Approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model: {'estimator': Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "      normalize=False, random_state=None, solver='auto', tol=0.001)}\n",
      "RMSE: 100.0147387841499\n",
      "R^2: 0.43635167861691004\n"
     ]
    }
   ],
   "source": [
    "#x_train, x_test, y_train, y_test = train_test_split(features, label, test_size = 0.2, random_state = 0)\n",
    "\n",
    "#x_train, y_train = delete_price_outliers(x_train, y_train)\n",
    "\n",
    "# test different regression approaches\n",
    "estimators = [ LinearRegression(), Ridge(), KNeighborsRegressor(), DecisionTreeRegressor(), MLPRegressor(), SVR() ]\n",
    "svr = [ SVR() ]\n",
    "pipeline = Pipeline( [ ('preprocessing', StandardScaler()), ('estimator', None) ])\n",
    "\n",
    "# define a parameter grid\n",
    "parameters = {\n",
    "    'estimator': estimators\n",
    "}\n",
    "\n",
    "# define and run a grid search using MSE as scoring metric\n",
    "search = GridSearchCV(pipeline, parameters, cv=10, scoring='neg_mean_squared_error')\n",
    "search.fit(x_train, y_train)\n",
    "\n",
    "# evaluate on test set\n",
    "predictions = search.predict(x_test)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "\n",
    "print(\"Best Model: {}\".format(search.best_params_))\n",
    "print(\"RMSE: {}\".format(sqrt(mse)))\n",
    "print(\"R^2: {}\".format(r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Evaluation of the Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run modules/evaluation.py\n",
    "\n",
    "best_r2 = 0\n",
    "\n",
    "# k-fold cross-validation (k = 10)\n",
    "scores = []\n",
    "k_fold_cross_validation = KFold(10, True, 1)\n",
    "for train_index, test_index in k_fold_cross_validation.split(features):\n",
    "\n",
    "    # Split the dataset for training and testing\n",
    "    x_train, x_test, y_train, y_test = features.loc[train_index, :], features.loc[test_index, :], label[train_index], label[test_index]\n",
    "\n",
    "    # Support Vector Regressor (SVR) using training dataset\n",
    "    svr = SVR(kernel='linear', C = 0.7)\n",
    "    svr.fit(x_train, y_train)\n",
    "\n",
    "    # Evaluation using testing dataset\n",
    "    scores.append(svr.score(x_test, y_test))  \n",
    "\n",
    "# Calculate performance measures\n",
    "print(\"r2: \", str(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-71bd6feefa3e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;31m# Support Vector Regressor (SVR) using training dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0msvr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSVR\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'linear'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0msvr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;31m# Evaluation using testing dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    144\u001b[0m         X, y = check_X_y(X, y, dtype=np.float64,\n\u001b[0;32m    145\u001b[0m                          \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'C'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m                          accept_large_sparse=False)\n\u001b[0m\u001b[0;32m    147\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    717\u001b[0m                     \u001b[0mensure_min_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m                     \u001b[0mwarn_on_dtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwarn_on_dtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 719\u001b[1;33m                     estimator=estimator)\n\u001b[0m\u001b[0;32m    720\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    541\u001b[0m             _assert_all_finite(array,\n\u001b[1;32m--> 542\u001b[1;33m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[0;32m    543\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan)\u001b[0m\n\u001b[0;32m     54\u001b[0m                 not allow_nan and not np.isfinite(X).all()):\n\u001b[0;32m     55\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'infinity'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m'NaN, infinity'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype_err\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m     \u001b[1;31m# for object dtype data, we only check for NaNs (GH-13254)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'object'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "%run modules/evaluation.py\n",
    "\n",
    "\n",
    "best_r2 = 0\n",
    "\n",
    "# Generate all feature combinations\n",
    "feature_combinations = generate_feature_combinations(already_preprocessed)\n",
    "\n",
    "for feature_combination in feature_combinations:\n",
    "    \n",
    "    # Filter the selected features\n",
    "    selected_features = already_preprocessed[feature_combination]\n",
    "    \n",
    "    # k-fold cross-validation (k = 10)\n",
    "    scores = []\n",
    "    k_fold_cross_validation = KFold(10, True, 1)\n",
    "    for train_index, test_index in k_fold_cross_validation.split(selected_features):\n",
    "    \n",
    "        # Split the dataset for training and testing\n",
    "        x_train, x_test, y_train, y_test = selected_features.loc[train_index, :], selected_features.loc[test_index, :], label[train_index], label[test_index]\n",
    "\n",
    "        # Support Vector Regressor (SVR) using training dataset\n",
    "        svr = SVR(kernel='linear', C = 0.7)\n",
    "        svr.fit(x_train, y_train)\n",
    "\n",
    "        # Evaluation using testing dataset\n",
    "        scores.append(svr.score(x_test, y_test))  \n",
    "\n",
    "    # Calculate performance measures\n",
    "    print(np.mean(scores), \" - \", feature_combination)\n",
    "\n",
    "    # Save best model\n",
    "    if(np.mean(scores) > best_r2):\n",
    "          best_r2 = np.mean(scores)\n",
    "        \n",
    "print(\"Best: \", best_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Evaluation of Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 12205.405453113695\n",
      "RMSE: 110.47807679858342\n",
      "R^2: 0.39620656940976784\n"
     ]
    }
   ],
   "source": [
    "#, stratify=y_binned\n",
    "#, random_state = 42\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, label, test_size = 0.2, random_state = 42, stratify=bins)\n",
    "\n",
    "#x_train, y_train = delete_price_outliers(x_train, y_train)\n",
    "\n",
    "reg = LinearRegression()\n",
    "reg.fit(x_train, y_train)\n",
    "prediction = reg.predict(x_test)\n",
    "mse = mean_squared_error(y_test, prediction)\n",
    "r2 = r2_score(y_test, prediction)\n",
    "print(\"MSE:\", mse)\n",
    "print(\"RMSE:\", sqrt(mse))\n",
    "print(\"R^2:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Evaluation of Advanced Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 20}\n",
      "-4696.814613726855\n"
     ]
    }
   ],
   "source": [
    "#1. Ridge Regression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "rr_est = Ridge()\n",
    "param = {\"alpha\": [1e-10, 1e-8, 1e-4,1e-3, 1e-2, 1, 5, 10, 20]}\n",
    "\n",
    "rr_est_opt = GridSearchCV(rr_est, param, scoring=\"neg_mean_squared_error\", cv=5)\n",
    "rr_est_opt.fit(x_train, y_train)\n",
    "\n",
    "# Get best param\n",
    "print(rr_est_opt.best_params_)\n",
    "print(rr_est_opt.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 11276.885237588422\n",
      "RMSE: 106.19267977402407\n",
      "R^2: 0.3831739616006318\n"
     ]
    }
   ],
   "source": [
    "# Test with alpha 20\n",
    "from sklearn.linear_model import ridge\n",
    "rr = Ridge(alpha = 20)\n",
    "rr.fit(x_train, y_train)\n",
    "rr.coef_\n",
    "\n",
    "#Test and Evaluate\n",
    "rr_predictions = rr.predict(x_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, rr_predictions)\n",
    "r2 = r2_score(y_test, rr_predictions)\n",
    "\n",
    "print(\"MSE:\", mse)\n",
    "print(\"RMSE:\", sqrt(mse))\n",
    "print(\"R^2:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6. Evaluation of KNN Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate development of error term with different values for K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative Mean squared error:  -21477.76092454774 for a K of 1\n",
      "Negative Mean squared error:  -18172.159012237866 for a K of 2\n",
      "Negative Mean squared error:  -17441.135494347203 for a K of 3\n",
      "Negative Mean squared error:  -16850.657433589662 for a K of 4\n",
      "Negative Mean squared error:  -16791.13280286299 for a K of 5\n",
      "Negative Mean squared error:  -16765.48719289242 for a K of 6\n",
      "Negative Mean squared error:  -16734.8804725024 for a K of 7\n",
      "Negative Mean squared error:  -16628.455845007862 for a K of 8\n",
      "Negative Mean squared error:  -16626.042864958483 for a K of 9\n",
      "Negative Mean squared error:  -16627.058175746464 for a K of 10\n",
      "Negative Mean squared error:  -16611.733558632426 for a K of 11\n",
      "Negative Mean squared error:  -16613.513410009924 for a K of 12\n",
      "Negative Mean squared error:  -16660.906663192593 for a K of 13\n",
      "Negative Mean squared error:  -16697.553606518184 for a K of 14\n",
      "Negative Mean squared error:  -16750.762711447842 for a K of 15\n",
      "Negative Mean squared error:  -16744.852812006735 for a K of 16\n",
      "Negative Mean squared error:  -16721.63708761064 for a K of 17\n",
      "Negative Mean squared error:  -16774.11865973724 for a K of 18\n",
      "Negative Mean squared error:  -16785.511451063172 for a K of 19\n",
      "Negative Mean squared error:  -16803.709643445214 for a K of 20\n",
      "Negative Mean squared error:  -16824.827800683568 for a K of 21\n",
      "Negative Mean squared error:  -16852.967029045303 for a K of 22\n",
      "Negative Mean squared error:  -16864.76491645321 for a K of 23\n",
      "Negative Mean squared error:  -16897.75603088103 for a K of 24\n",
      "Negative Mean squared error:  -16908.666332627938 for a K of 25\n",
      "Negative Mean squared error:  -16894.59355594266 for a K of 26\n",
      "Negative Mean squared error:  -16934.838377053366 for a K of 27\n",
      "Negative Mean squared error:  -16947.002892454075 for a K of 28\n"
     ]
    }
   ],
   "source": [
    "%run modules/evaluation.py\n",
    "\n",
    "best_nmse = -100000\n",
    "\n",
    "neg_mse = []\n",
    "for K in range(30):\n",
    "    K = K + 1\n",
    "    knn = neighbors.KNeighborsRegressor(n_neighbors=K)\n",
    "\n",
    "    # k-fold cross-validation (k = 10)\n",
    "    scores = []\n",
    "    k_fold_cross_validation = KFold(10, True, 1)\n",
    "    for train_index, test_index in k_fold_cross_validation.split(features):\n",
    "\n",
    "        # KNN Regression using training dataset\n",
    "        knn = neighbors.KNeighborsRegressor(n_neighbors=K)\n",
    "        knn.fit(x_train, y_train)\n",
    "\n",
    "        # Evaluation using testing dataset\n",
    "        scores.append(cross_val_score(knn, x_train, y_train, scoring='neg_mean_squared_error'))  \n",
    "\n",
    "    # Calculate performance measures\n",
    "    print(\"Negative mean squared error: \", str(np.mean(scores)), \"for a K of\", K)\n",
    "    neg_mse.append(np.mean(scores))\n",
    "    \n",
    "    # Save best model\n",
    "    if(np.mean(scores) > best_nmse):\n",
    "          best_nmse = np.mean(scores)\n",
    "            \n",
    "# plotting the rmse values against k values\n",
    "curve = px.line(data, x=\"Value of K\", y=\"Negative Mean Squared Error\", title='Change of NMSE for different values of K')\n",
    "curve.show()\n",
    "\n",
    "\n",
    "print(\"Best: \", best_nmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GridSearch to evaluate the best parameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model: {'metric': 'manhattan', 'n_neighbors': 12, 'weights': 'distance'}\n",
      "RMSE: 130.14655641574794\n",
      "R^2: 0.16208196988783952\n",
      "best score is -15456.122970596134 with params {'metric': 'manhattan', 'n_neighbors': 12, 'weights': 'distance'}\n"
     ]
    }
   ],
   "source": [
    "# create an estimator\n",
    "knn_estimator = neighbors.KNeighborsRegressor()\n",
    "parameters = {\n",
    "    'n_neighbors': range(2, 30),\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['manhattan', 'euclidean', 'minkowski', 'chebyshev']\n",
    "}\n",
    "\n",
    "# specify the cross validation\n",
    "k_fold_cross_validation = KFold(10, True, 1)\n",
    "\n",
    "grid_search_estimator = GridSearchCV(knn_estimator, parameters, cv=k_fold_cross_validation, scoring='neg_mean_squared_error')\n",
    "grid_search_estimator.fit(x_train,y_train)\n",
    "grid_search_estimator.best_params_\n",
    "\n",
    "# evaluate on test set\n",
    "predictions = grid_search_estimator.predict(x_test)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "\n",
    "print(\"Best Model: {}\".format(grid_search_estimator.best_params_))\n",
    "print(\"RMSE: {}\".format(sqrt(mse)))\n",
    "print(\"R^2: {}\".format(r2))\n",
    "\n",
    "\n",
    "# print the best parameter setting\n",
    "print(\"best score is {} with params {}\".format(grid_search_estimator.best_score_, grid_search_estimator.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tryout different feature combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run modules/evaluation.py\n",
    "\n",
    "best_r2 = 0\n",
    "best_feature_combi = 0\n",
    "\n",
    "# Generate all feature combinations\n",
    "feature_combinations = generate_feature_combinations(features)\n",
    "\n",
    "for feature_combination in feature_combinations:\n",
    "    # Filter the selected features\n",
    "    \n",
    "    x_train = x_train[feature_combination]\n",
    "    y_train = y_train[feature_combination]\n",
    "            \n",
    "    # Tryout with manhatten\n",
    "    knn = KNeighborsRegressor(n_neighbors = 9, metric = 'manhattan', weights= 'uniform')\n",
    "    knn.fit(x_train, y_train)\n",
    "    price_predicted = knn.predict(x_test)\n",
    "\n",
    "    # evaluate using different measures\n",
    "    mse = mean_squared_error(y_test, price_predicted)\n",
    "    r2 = r2_score(y_test, price_predicted)\n",
    "        \n",
    "    print(\"Evaluation of feature combination\", feature_combination)\n",
    "    print(\"MSE:\", mse)\n",
    "    print(\"RMSE:\", sqrt(mse))\n",
    "    print(\"R^2:\", r2)\n",
    "        \n",
    "    if(r2 > best_r2):\n",
    "        best_r2 = r2\n",
    "        best_feature_combi = feature_combination\n",
    "            \n",
    "    print(\"Best score so far:\", best_r2, \"with the features:\", best_feature_combi)\n",
    "        \n",
    "print(\"Best score overall: \", best_r2, \"with the features:\", best_feature_combi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.7 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest:\n",
      "Fitting 10 folds for each of 320 candidates, totalling 3200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   10.0s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   42.6s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1242 tasks      | elapsed:  7.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1792 tasks      | elapsed: 10.8min\n",
      "[Parallel(n_jobs=-1)]: Done 2442 tasks      | elapsed: 15.3min\n",
      "[Parallel(n_jobs=-1)]: Done 3192 tasks      | elapsed: 20.7min\n",
      "[Parallel(n_jobs=-1)]: Done 3200 out of 3200 | elapsed: 20.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model: {'regressor__max_depth': 11, 'regressor__max_leaf_nodes': 47, 'regressor__min_samples_leaf': 16, 'regressor__min_samples_split': 27, 'regressor__n_estimators': 100, 'regressor__n_jobs': -1}\n",
      "Training performance:\n",
      "RMSE: 107.34253733421592\n",
      "R^2: 0.41833627515486294\n",
      "\n",
      "Test performance:\n",
      "RMSE: 109.69567985787793\n",
      "R^2: 0.40472832096896927\n"
     ]
    }
   ],
   "source": [
    "#Random Forest\n",
    "print(\"Random Forest:\")\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, label, test_size = 0.2, random_state = 42, stratify=bins)\n",
    "#x_train, y_train = delete_price_outliers(x_train, y_train)\n",
    "\n",
    "\n",
    "rf_regressor = RandomForestRegressor()\n",
    "\n",
    "#define ranges for parameters\n",
    "max_depth = np.arange(5,15,2)\n",
    "min_samples_split = np.arange(2,100,25)\n",
    "min_samples_leafs = np.arange(1,50,15)\n",
    "max_leaf_nodes = np.arange(2,50,15)\n",
    "\n",
    "\n",
    "pipeline = Pipeline( [ ('preprocessing', StandardScaler()), ('regressor', rf_regressor) ])\n",
    "\n",
    "\n",
    "# define and run a grid search using r2 as scoring metric\n",
    "parameters = {'regressor__n_estimators': [100],\n",
    "                'regressor__max_depth': max_depth,\n",
    "                'regressor__n_jobs': [-1],\n",
    "                'regressor__min_samples_leaf': min_samples_leafs,\n",
    "                   'regressor__min_samples_split': min_samples_split,\n",
    "                'regressor__max_leaf_nodes': max_leaf_nodes}\n",
    "\n",
    "# define and run a grid search using r2 as scoring metric\n",
    "rf_search = GridSearchCV(pipeline, parameters, cv=10, scoring='r2', verbose=1, n_jobs=-1)\n",
    "rf_search.fit(x_train, y_train)\n",
    "\n",
    "#Training Error\n",
    "print(f\"Best Model: {rf_search.best_params_}\")\n",
    "training_predictions = rf_search.predict(x_train)\n",
    "training_mse = mean_squared_error(y_train, training_predictions)\n",
    "training_r2 = r2_score(y_train, training_predictions)\n",
    "print(\"Training performance:\")\n",
    "print(f\"RMSE: {sqrt(training_mse)}\")\n",
    "print(f\"R^2: {training_r2}\")\n",
    "\n",
    "print()\n",
    "\n",
    "#Test Error\n",
    "test_predictions = rf_search.predict(x_test)\n",
    "test_mse = mean_squared_error(y_test, test_predictions)\n",
    "test_r2 = r2_score(y_test, test_predictions)\n",
    "print(\"Test performance:\")\n",
    "print(f\"RMSE: {sqrt(test_mse)}\")\n",
    "print(f\"R^2: {test_r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='interpretation_evaluation'></a>\n",
    "## 3 Interpretation and Evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
